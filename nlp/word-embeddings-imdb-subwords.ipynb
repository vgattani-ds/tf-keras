{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers as L\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Difference betweeen Embedding Layer and Dense Layer\n",
    "    * https://medium.com/logivan/neural-network-embedding-and-dense-layers-whats-the-difference-fa177c6d0304\n",
    "    \n",
    "2. Source\n",
    "    * https://www.tensorflow.org/tutorials/text/word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does Different Layers do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers as L\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 2), dtype=float32, numpy=\n",
       "array([[[ 0.02515623, -0.03187831],\n",
       "        [-0.01517564,  0.00728263],\n",
       "        [-0.00040847,  0.02596793]],\n",
       "\n",
       "       [[ 0.03853666, -0.02476275],\n",
       "        [-0.00047541, -0.04866255],\n",
       "        [ 0.00736698, -0.03209202]]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = L.Embedding(10, 2)\n",
    "embedding(np.array([[0,1,9],[5,3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 0.00319071,  0.00045742],\n",
       "       [ 0.01514274, -0.03517244]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling = L.GlobalAveragePooling1D()\n",
    "pooling(embedding(np.array([[0,1,9],[5,3,4]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[-0.00336104, -0.00394651,  0.01277973],\n",
       "       [ 0.00688695, -0.02456898, -0.01236252]], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooling = L.GlobalAveragePooling1D(data_format='channels_first')\n",
    "pooling(embedding(np.array([[0,1,9],[5,3,4]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers as L\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
     ]
    }
   ],
   "source": [
    "(train_data, test_data), info = tfds.load('imdb_reviews/subwords8k',\n",
    "                                          split = (tfds.Split.TRAIN, tfds.Split.TEST),\n",
    "                                          with_info=True, as_supervised=True)\n",
    "\n",
    "encoder = info.features['text'].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = train_data.shuffle(1000).padded_batch(10, padded_shapes=([None],[]))\n",
    "test_batches = test_data.shuffle(1000).padded_batch(10, padded_shapes=([None],[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 519,  198,   46, ...,    0,    0,    0],\n",
       "       [7984, 7986, 7976, ...,    0,    0,    0],\n",
       "       [3187,   89, 6545, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 968,   47, 3142, ...,    0,    0,    0],\n",
       "       [ 601, 6144, 8002, ...,    0,    0,    0],\n",
       "       [ 156,  151, 7968, ...,    6, 1405, 7975]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch, train_labels = next(iter(train_batches))\n",
    "train_batch.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded text: [  62   18   41  604  927   65    3  644 7968   21]\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "for train_example, train_label in train_data.take(1):\n",
    "    print('Encoded text:', train_example[:10].numpy())\n",
    "    print('Label:', train_label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (10, 1352)\n",
      "label shape: (10,)\n",
      "Batch shape: (10, 712)\n",
      "label shape: (10,)\n"
     ]
    }
   ],
   "source": [
    "for example_batch, label_batch in train_batches.take(2):\n",
    "    print(\"Batch shape:\", example_batch.shape)\n",
    "    print(\"label shape:\", label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DIM = 16\n",
    "VOCAB_SIZE = encoder.vocab_size\n",
    "\n",
    "model = keras.Sequential([\n",
    "    L.Embedding(VOCAB_SIZE, EMBEDDING_DIM),\n",
    "    L.GlobalAveragePooling1D(),\n",
    "    L.Dense(16, activation='relu'),\n",
    "    L.Dense(1)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 16)          130960    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 131,249\n",
      "Trainable params: 131,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*16 + 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130960"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE*EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8185"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2500/2500 [==============================] - 8s 3ms/step - loss: 0.5019 - accuracy: 0.7017 - val_loss: 0.3794 - val_accuracy: 0.8500\n",
      "Epoch 2/5\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.2814 - accuracy: 0.8853 - val_loss: 0.2552 - val_accuracy: 0.9050\n",
      "Epoch 3/5\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.2293 - accuracy: 0.9118 - val_loss: 0.3353 - val_accuracy: 0.8850\n",
      "Epoch 4/5\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.1953 - accuracy: 0.9243 - val_loss: 0.3647 - val_accuracy: 0.8850\n",
      "Epoch 5/5\n",
      "2500/2500 [==============================] - 5s 2ms/step - loss: 0.1728 - accuracy: 0.9362 - val_loss: 0.3565 - val_accuracy: 0.8550\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "        train_batches,\n",
    "        epochs=5,\n",
    "        validation_data=test_batches,\n",
    "        validation_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info.description\n",
    "#info.download_size\n",
    "#info.supervised_keys\n",
    "#info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.embeddings.Embedding at 0x13b3351d0>,\n",
       " <tensorflow.python.keras.layers.pooling.GlobalAveragePooling1D at 0x10330fb90>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x13c40d910>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x13c34af90>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8185, 16)\n"
     ]
    }
   ],
   "source": [
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.choice(['A','B','C','D'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "out_v = io.open('../big-files/word-embeddings-vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('../big-files/word-embeddings-meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "out_m.write(\"\\t\".join([\"word\",\"class\"]) + \"\\n\")\n",
    "\n",
    "for num, word in enumerate(encoder.subwords):\n",
    "    vec = weights[num+1] # skip 0, it's padding.\n",
    "    out_m.write(\"\\t\".join([word,random.choice(['A','B','C','D'])]) + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the embeddings\n",
    "To visualize our embeddings upload them to the embedding projector.\n",
    "\n",
    "Open the Embedding Projector (this can also run in a local TensorBoard instance).\n",
    "\n",
    "* Click on \"Load data\".\n",
    "\n",
    "* Upload the two files created above: vecs.tsv and meta.tsv.\n",
    "\n",
    "The embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tclass\r\n",
      "the_\tC\r\n",
      ", \tB\r\n",
      ". \tB\r\n",
      "a_\tA\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 word-embeddings-meta.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09168589\t-0.03764703\t0.13426588\t0.045826133\t0.10915152\t0.12124654\t0.057822347\t0.105642885\t0.06465526\t-0.08695891\t0.0044927956\t0.060063865\t-0.048089758\t0.009972526\t0.017446194\t0.05941992\r\n",
      "-0.011089035\t-0.02632265\t0.08001026\t0.026919086\t0.020393694\t0.11729438\t-0.025492614\t0.057734393\t0.073117696\t-0.07000069\t-0.010621369\t0.0038523786\t0.0051375018\t0.0068875127\t0.017043302\t-0.036084823\r\n",
      "-0.014711189\t-0.0068150936\t-0.021027738\t0.12178983\t-0.010772438\t0.077707574\t-0.05735729\t0.02820964\t0.09139164\t0.031960886\t-0.0079053715\t0.047761936\t0.030932683\t0.0064102286\t0.030124169\t-0.08493775\r\n",
      "-0.03751501\t-0.032466944\t0.041288\t0.09144153\t0.064260535\t0.095885985\t0.0018964682\t0.117076196\t0.05275915\t0.016736258\t-0.048899982\t0.010198308\t-0.0040065846\t-0.0042498633\t-0.001717417\t0.00035502313\r\n",
      "0.105565146\t-0.040354658\t0.13522404\t-0.010742345\t0.15480578\t0.16393423\t0.036755394\t0.10789068\t0.1774977\t-0.1026288\t0.010162583\t0.0900577\t-0.09602754\t-0.076360814\t0.07845271\t0.062755905\r\n"
     ]
    }
   ],
   "source": [
    "!head -5 word-embeddings-vecs.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7928"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoder.subwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8185, 16)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.subwords.index('the_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[466,\n",
       " 8042,\n",
       " 5191,\n",
       " 5191,\n",
       " 5191,\n",
       " 7978,\n",
       " 5656,\n",
       " 1096,\n",
       " 8035,\n",
       " 1647,\n",
       " 8042,\n",
       " 8035,\n",
       " 7554,\n",
       " 8048,\n",
       " 8035]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_a = encoder.encode(\"theyq1111111opiopjwoqjdoiwj\")\n",
    "_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theyq1111111opiopjwoqjdoiwj'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join([encoder.decode([x]) for x in _a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as_', '10_', 'is_', 'yell']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[encoder.subwords[x] for x in [19, 466, 8, 5191]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.decode([7929])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x01'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.decode([7930])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.decode([8024])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.decode([8184])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.decode([8183])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
